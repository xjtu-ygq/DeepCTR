{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f45d018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/ygq/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1426b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './criteo.csv'\n",
    "model_name='widedeep'\n",
    "epoch = 100\n",
    "learning_rate = 1e-3\n",
    "batch_size=2048\n",
    "weight_decay=1e-3\n",
    "save_dir='./'\n",
    "seed=2023\n",
    "# device='cpu'\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed14cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auto_embedding_dim(num_classes):\n",
    "    return np.floor(6 * np.pow(num_classes, 0.26))\n",
    "\n",
    "class SequenceFeature(object):\n",
    "    def __init__(self, name, vocab_size, embed_dim=None, pooling=\"mean\", shared_with=None):\n",
    "        self.name = name\n",
    "        self.vocab_size = vocab_size\n",
    "        if embed_dim == None:\n",
    "            self.embed_dim = get_auto_embedding_dim(vocab_size)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "        self.pooling = pooling\n",
    "        self.shared_with = shared_with\n",
    "        \n",
    "class SparseFeature(object):\n",
    "    def __init__(self, name, vocab_size, embed_dim=None, shared_with=None):\n",
    "        self.name = name\n",
    "        self.vocab_size = vocab_size\n",
    "        if embed_dim == None:\n",
    "            self.embed_dim = get_auto_embedding_dim(vocab_size)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "        self.shared_with = shared_with\n",
    "\n",
    "class DenseFeature(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.embed_dim = 1\n",
    "        \n",
    "def convert_numeric_feature(val):\n",
    "    v = int(val)\n",
    "    if v > 2:\n",
    "        return int(np.log(v)**2)\n",
    "    else:\n",
    "        return v - 2\n",
    "    \n",
    "def get_criteo_data_dict(data_path):\n",
    "    if data_path.endswith(\".gz\"):  #if the raw_data is gz file:\n",
    "        data = pd.read_csv(data_path, compression=\"gzip\")\n",
    "    else:\n",
    "        data = pd.read_csv(data_path,nrows=100000)\n",
    "    print(\"data load finished\")\n",
    "    dense_features = [f for f in data.columns.tolist() if f[0] == \"I\"]\n",
    "    sparse_features = [f for f in data.columns.tolist() if f[0] == \"C\"]\n",
    "\n",
    "    data[sparse_features] = data[sparse_features].fillna('-996',)\n",
    "    data[dense_features] = data[dense_features].fillna(0,)\n",
    "\n",
    "    for feat in tqdm(dense_features):  #discretize dense feature and as new sparse feature\n",
    "        sparse_features.append(feat + \"_cat\")\n",
    "        data[feat + \"_cat\"] = data[feat].apply(lambda x: convert_numeric_feature(x))\n",
    "\n",
    "    sca = MinMaxScaler()  #scaler dense feature\n",
    "    data[dense_features] = sca.fit_transform(data[dense_features])\n",
    "\n",
    "    for feat in tqdm(sparse_features):  #encode sparse feature\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "    dense_feas = [DenseFeature(feature_name) for feature_name in dense_features]\n",
    "    sparse_feas = [SparseFeature(feature_name, vocab_size=data[feature_name].nunique(), embed_dim=16) for feature_name in sparse_features]\n",
    "    y = data[\"label\"]\n",
    "    del data[\"label\"]\n",
    "    x = data\n",
    "    return dense_feas, sparse_feas, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc8e450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cccf9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {k: v[index] for k, v in self.x.items()}, self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "class DataGenerator(object):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.dataset = TorchDataset(x, y)\n",
    "        self.length = len(self.dataset)\n",
    "\n",
    "    def generate_dataloader(self,\n",
    "                            x_val=None,\n",
    "                            y_val=None,\n",
    "                            x_test=None,\n",
    "                            y_test=None,\n",
    "                            split_ratio=None,\n",
    "                            batch_size=16,\n",
    "                            num_workers=8):\n",
    "        if split_ratio != None:\n",
    "            train_length = int(self.length * split_ratio[0])\n",
    "            val_length = int(self.length * split_ratio[1])\n",
    "            test_length = self.length - train_length - val_length\n",
    "            print(\"the samples of train : val : test are  %d : %d : %d\" % (train_length, val_length, test_length))\n",
    "            train_dataset, val_dataset, test_dataset = random_split(self.dataset, (train_length, val_length, test_length))\n",
    "        else:\n",
    "            train_dataset = self.dataset\n",
    "            val_dataset = TorchDataset(x_val, y_val)\n",
    "            test_dataset = TorchDataset(x_test, y_test)\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "        return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af5f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4aaf4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  6.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 39/39 [00:01<00:00, 27.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the samples of train : val : test are  70000 : 10000 : 20000\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "dense_feas, sparse_feas, x, y = get_criteo_data_dict(dataset_path)\n",
    "\n",
    "dg = DataGenerator(x, y)\n",
    "train_dataloader, val_dataloader, test_dataloader = dg.generate_dataloader(split_ratio=[0.7, 0.1], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b16e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self, input_dim, sigmoid=False):\n",
    "        super().__init__()\n",
    "        self.sigmoid = sigmoid\n",
    "        self.fc = nn.Linear(input_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.sigmoid:\n",
    "            return torch.sigmoid(self.fc(x))\n",
    "        else:\n",
    "            return self.fc(x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_layer=True, dims=[], dropout=0, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        layers = list()\n",
    "        for i_dim in dims:\n",
    "            layers.append(nn.Linear(input_dim, i_dim))\n",
    "            layers.append(nn.BatchNorm1d(i_dim))\n",
    "            layers.append(activation_layer(activation))\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            input_dim = i_dim\n",
    "        if output_layer:\n",
    "            layers.append(nn.Linear(input_dim, 1))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.embed_dict = nn.ModuleDict()\n",
    "        self.n_dense = 0\n",
    "\n",
    "        for fea in features:\n",
    "            if fea.name in self.embed_dict:  #exist\n",
    "                continue\n",
    "            if isinstance(fea, SparseFeature) and fea.shared_with == None:\n",
    "                self.embed_dict[fea.name] = nn.Embedding(fea.vocab_size, fea.embed_dim)\n",
    "            elif isinstance(fea, SequenceFeature) and fea.shared_with == None:\n",
    "                self.embed_dict[fea.name] = nn.Embedding(fea.vocab_size, fea.embed_dim)\n",
    "            elif isinstance(fea, DenseFeature):\n",
    "                self.n_dense += 1\n",
    "        for matrix in self.embed_dict.values():  #init embedding weight\n",
    "            torch.nn.init.xavier_normal_(matrix.weight)\n",
    "\n",
    "    def forward(self, x, features, squeeze_dim=False):\n",
    "        sparse_emb, dense_values = [], []\n",
    "        sparse_exists, dense_exists = False, False\n",
    "        for fea in features:\n",
    "            if isinstance(fea, SparseFeature):\n",
    "                if fea.shared_with == None:\n",
    "                    sparse_emb.append(self.embed_dict[fea.name](x[fea.name].long()).unsqueeze(1))\n",
    "                else:\n",
    "                    sparse_emb.append(self.embed_dict[fea.shared_with](x[fea.name].long()).unsqueeze(1))\n",
    "            elif isinstance(fea, SequenceFeature):\n",
    "                if fea.pooling == \"sum\":\n",
    "                    pooling_layer = SumPooling()\n",
    "                elif fea.pooling == \"mean\":\n",
    "                    pooling_layer = AveragePooling()\n",
    "                elif fea.pooling == \"concat\":\n",
    "                    pooling_layer = ConcatPooling()\n",
    "                else:\n",
    "                    raise ValueError(\"Sequence pooling method supports only pooling in %s, got %s.\" %\n",
    "                                     ([\"sum\", \"mean\"], fea.pooling))\n",
    "                if fea.shared_with == None:\n",
    "                    sparse_emb.append(pooling_layer(self.embed_dict[fea.name](x[fea.name].long())).unsqueeze(1))\n",
    "                else:\n",
    "                    sparse_emb.append(pooling_layer(self.embed_dict[fea.shared_with](\n",
    "                        x[fea.name].long())).unsqueeze(1))  #shared specific sparse feature embedding\n",
    "            else:\n",
    "                dense_values.append(x[fea.name].float().unsqueeze(1))  #.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        if len(dense_values) > 0:\n",
    "            dense_exists = True\n",
    "            dense_values = torch.cat(dense_values, dim=1)\n",
    "        if len(sparse_emb) > 0:\n",
    "            sparse_exists = True\n",
    "            sparse_emb = torch.cat(sparse_emb, dim=1)  #[batch_size, num_features, embed_dim]\n",
    "\n",
    "        if squeeze_dim:  #Note: if the emb_dim of sparse features is different, we must squeeze_dim\n",
    "            if dense_exists and not sparse_exists:  #only input dense features\n",
    "                return dense_values\n",
    "            elif not dense_exists and sparse_exists:\n",
    "                return sparse_emb.flatten(start_dim=1)  #squeeze dim to : [batch_size, num_features*embed_dim]\n",
    "            elif dense_exists and sparse_exists:\n",
    "                return torch.cat((sparse_emb.flatten(start_dim=1), dense_values),\n",
    "                                 dim=1)  #concat dense value with sparse embedding\n",
    "            else:\n",
    "                raise ValueError(\"The input features can note be empty\")\n",
    "        else:\n",
    "            if sparse_exists:\n",
    "                return sparse_emb  #[batch_size, num_features, embed_dim]\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"If keep the original shape:[batch_size, num_features, embed_dim], expected %s in feature list, got %s\" %\n",
    "                    (\"SparseFeatures\", features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ec3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dice(nn.Module):\n",
    "    \"\"\"The Dice activation function mentioned in the `DIN paper\n",
    "    https://arxiv.org/abs/1706.06978`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-3):\n",
    "        super(Dice, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: N * num_neurons\n",
    "        avg = x.mean(dim=1)  # N\n",
    "        avg = avg.unsqueeze(dim=1)  # N * 1\n",
    "        var = torch.pow(x - avg, 2) + self.epsilon  # N * num_neurons\n",
    "        var = var.sum(dim=1).unsqueeze(dim=1)  # N * 1\n",
    "\n",
    "        ps = (x - avg) / torch.sqrt(var)  # N * 1\n",
    "\n",
    "        ps = nn.Sigmoid()(ps)  # N * 1\n",
    "        return ps * x + (1 - ps) * self.alpha * x\n",
    "\n",
    "\n",
    "def activation_layer(act_name):\n",
    "    \"\"\"Construct activation layers\n",
    "\n",
    "    Args:\n",
    "        act_name: str or nn.Module, name of activation function\n",
    "    \n",
    "    Returns:\n",
    "        act_layer: activation layer\n",
    "    \"\"\"\n",
    "    if isinstance(act_name, str):\n",
    "        if act_name.lower() == 'sigmoid':\n",
    "            act_layer = nn.Sigmoid()\n",
    "        elif act_name.lower() == 'relu':\n",
    "            act_layer = nn.ReLU(inplace=True)\n",
    "        elif act_name.lower() == 'dice':\n",
    "            act_layer = Dice()\n",
    "        elif act_name.lower() == 'prelu':\n",
    "            act_layer = nn.PReLU()\n",
    "        elif act_name.lower() == \"softmax\":\n",
    "            act_layer = nn.Softmax(dim=1)\n",
    "    elif issubclass(act_name, nn.Module):\n",
    "        act_layer = act_name()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return act_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe8b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeep(torch.nn.Module):\n",
    "    \"\"\"Wide & Deep Learning model.\n",
    "\n",
    "    Args:\n",
    "        wide_features (list): the list of `Feature Class`, training by the wide part module.\n",
    "        deep_features (list): the list of `Feature Class`, training by the deep part module.\n",
    "        mlp_params (dict): the params of the last MLP module, keys include:`{\"dims\":list, \"activation\":str, \"dropout\":float, \"output_layer\":bool`}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, wide_features, deep_features, mlp_params):\n",
    "        super(WideDeep, self).__init__()\n",
    "        self.wide_features = wide_features\n",
    "        self.deep_features = deep_features\n",
    "        self.wide_dims = sum([fea.embed_dim for fea in wide_features])\n",
    "        self.deep_dims = sum([fea.embed_dim for fea in deep_features])\n",
    "        self.linear = LR(self.wide_dims)\n",
    "        self.embedding = EmbeddingLayer(wide_features + deep_features)\n",
    "        self.mlp = MLP(self.deep_dims, **mlp_params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_wide = self.embedding(x, self.wide_features, squeeze_dim=True)  #[batch_size, wide_dims]\n",
    "        input_deep = self.embedding(x, self.deep_features, squeeze_dim=True)  #[batch_size, deep_dims]\n",
    "\n",
    "        y_wide = self.linear(input_wide)  #[batch_size, 1]\n",
    "        y_deep = self.mlp(input_deep)  #[batch_size, 1]\n",
    "        y = y_wide + y_deep\n",
    "        y = torch.sigmoid(y.squeeze(1))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0fc5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class EarlyStopper(object):\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\n",
    "        \n",
    "    Args:\n",
    "        patience (int): How long to wait after last time validation auc improved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience):\n",
    "        self.patience = patience\n",
    "        self.trial_counter = 0\n",
    "        self.best_auc = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def stop_training(self, val_auc, weights):\n",
    "        \"\"\"whether to stop training.\n",
    "\n",
    "        Args:\n",
    "            val_auc (float): auc score in val data.\n",
    "            weights (tensor): the weights of model\n",
    "        \"\"\"\n",
    "        if val_auc > self.best_auc:\n",
    "            self.best_auc = val_auc\n",
    "            self.trial_counter = 0\n",
    "            self.best_weights = copy.deepcopy(weights)\n",
    "            return False\n",
    "        elif self.trial_counter + 1 < self.patience:\n",
    "            self.trial_counter += 1\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bdcaa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class CTRTrainer(object):\n",
    "    \"\"\"A general trainer for single task learning.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): any multi task learning model.\n",
    "        optimizer_fn (torch.optim): optimizer function of pytorch (default = `torch.optim.Adam`).\n",
    "        optimizer_params (dict): parameters of optimizer_fn.\n",
    "        scheduler_fn (torch.optim.lr_scheduler) : torch scheduling class, eg. `torch.optim.lr_scheduler.StepLR`.\n",
    "        scheduler_params (dict): parameters of optimizer scheduler_fn.\n",
    "        n_epoch (int): epoch number of training.\n",
    "        earlystop_patience (int): how long to wait after last time validation auc improved (default=10).\n",
    "        device (str): `\"cpu\"` or `\"cuda:0\"`\n",
    "        gpus (list): id of multi gpu (default=[]). If the length >=1, then the model will wrapped by nn.DataParallel.\n",
    "        model_path (str): the path you want to save the model (default=\"./\"). Note only save the best weight in the validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params={\n",
    "            \"lr\": 1e-3,\n",
    "            \"weight_decay\": 1e-5\n",
    "        },\n",
    "        scheduler_fn=None,\n",
    "        scheduler_params=None,\n",
    "        n_epoch=10,\n",
    "        earlystop_patience=10,\n",
    "        device=\"cpu\",\n",
    "        gpus=[],\n",
    "        model_path=\"./\",\n",
    "    ):\n",
    "        self.model = model  # for uniform weights save method in one gpu or multi gpu\n",
    "        self.optimizer = optimizer_fn(self.model.parameters(), **optimizer_params)  #default optimizer\n",
    "        self.scheduler = None\n",
    "        if scheduler_fn is not None:\n",
    "            self.scheduler = scheduler_fn(self.optimizer, **scheduler_params)\n",
    "        self.criterion = torch.nn.BCELoss()  #default loss cross_entropy\n",
    "        self.evaluate_fn = roc_auc_score  #default evaluate function\n",
    "        self.n_epoch = n_epoch\n",
    "        self.early_stopper = EarlyStopper(patience=earlystop_patience)\n",
    "        self.device = torch.device(device)  #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.gpus = gpus\n",
    "        if len(gpus) > 1:\n",
    "            print('parallel running on these gpus:', gpus)\n",
    "            self.model = torch.nn.DataParallel(self.model, device_ids=gpus)\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def train_one_epoch(self, data_loader, log_interval=10):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        tk0 = tqdm.tqdm(data_loader, desc=\"train\", smoothing=0, mininterval=1.0)\n",
    "        for i, (x_dict, y) in enumerate(tk0):\n",
    "            x_dict = {k: v.to(self.device) for k, v in x_dict.items()}  #tensor to GPU\n",
    "            y = y.to(self.device)\n",
    "            y_pred = self.model(x_dict)\n",
    "            loss = self.criterion(y_pred, y.float())\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                tk0.set_postfix(loss=total_loss / log_interval)\n",
    "                total_loss = 0\n",
    "\n",
    "    def fit(self, train_dataloader, val_dataloader=None):\n",
    "        self.model.to(self.device)\n",
    "        for epoch_i in range(self.n_epoch):\n",
    "            print('epoch:', epoch_i)\n",
    "            self.train_one_epoch(train_dataloader)\n",
    "            if self.scheduler is not None:\n",
    "                if epoch_i % self.scheduler.step_size == 0:\n",
    "                    print(\"Current lr : {}\".format(self.optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "                self.scheduler.step()  #update lr in epoch level by scheduler\n",
    "            if val_dataloader:\n",
    "                auc = self.evaluate(self.model, val_dataloader)\n",
    "                print('epoch:', epoch_i, 'validation: auc:', auc)\n",
    "                if self.early_stopper.stop_training(auc, self.model.state_dict()):\n",
    "                    print(f'validation: best auc: {self.early_stopper.best_auc}')\n",
    "                    self.model.load_state_dict(self.early_stopper.best_weights)\n",
    "                    torch.save(self.early_stopper.best_weights, os.path.join(self.model_path,\n",
    "                                                                             \"model.pth\"))  #save best auc model\n",
    "                    break\n",
    "\n",
    "    def evaluate(self, model, data_loader):\n",
    "        model.eval()\n",
    "        targets, predicts = list(), list()\n",
    "        with torch.no_grad():\n",
    "            tk0 = tqdm.tqdm(data_loader, desc=\"validation\", smoothing=0, mininterval=1.0)\n",
    "            for i, (x_dict, y) in enumerate(tk0):\n",
    "                x_dict = {k: v.to(self.device) for k, v in x_dict.items()}\n",
    "                y = y.to(self.device)\n",
    "                y_pred = model(x_dict)\n",
    "                targets.extend(y.tolist())\n",
    "                predicts.extend(y_pred.tolist())\n",
    "        return self.evaluate_fn(targets, predicts)\n",
    "\n",
    "    def predict(self, model, data_loader):\n",
    "        model.eval()\n",
    "        predicts = list()\n",
    "        with torch.no_grad():\n",
    "            tk0 = tqdm.tqdm(data_loader, desc=\"predict\", smoothing=0, mininterval=1.0)\n",
    "            for i, (x_dict, y) in enumerate(tk0):\n",
    "                x_dict = {k: v.to(self.device) for k, v in x_dict.items()}\n",
    "                y = y.to(self.device)\n",
    "                y_pred = model(x_dict)\n",
    "                predicts.extend(y_pred.tolist())\n",
    "        return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d93409",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SequenceFeature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWideDeep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwide_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_feas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_feas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdims\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [9], line 17\u001b[0m, in \u001b[0;36mWideDeep.__init__\u001b[0;34m(self, wide_features, deep_features, mlp_params)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeep_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([fea\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;28;01mfor\u001b[39;00m fea \u001b[38;5;129;01min\u001b[39;00m deep_features])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m LR(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwide_dims)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m \u001b[43mEmbeddingLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwide_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdeep_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m MLP(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeep_dims, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmlp_params)\n",
      "Cell \u001b[0;32mIn [7], line 42\u001b[0m, in \u001b[0;36mEmbeddingLayer.__init__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fea, SparseFeature) \u001b[38;5;129;01mand\u001b[39;00m fea\u001b[38;5;241m.\u001b[39mshared_with \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dict[fea\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(fea\u001b[38;5;241m.\u001b[39mvocab_size, fea\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fea, \u001b[43mSequenceFeature\u001b[49m) \u001b[38;5;129;01mand\u001b[39;00m fea\u001b[38;5;241m.\u001b[39mshared_with \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dict[fea\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(fea\u001b[38;5;241m.\u001b[39mvocab_size, fea\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fea, DenseFeature):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SequenceFeature' is not defined"
     ]
    }
   ],
   "source": [
    "model = WideDeep(wide_features=dense_feas, deep_features=sparse_feas, mlp_params={\"dims\": [256, 128], \"dropout\": 0.2, \"activation\": \"relu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee6a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_trainer = CTRTrainer(model, optimizer_params={\"lr\": learning_rate, \"weight_decay\": weight_decay}, n_epoch=epoch, earlystop_patience=10, device=device, model_path=save_dir)\n",
    "#scheduler_fn=torch.optim.lr_scheduler.StepLR,scheduler_params={\"step_size\": 2,\"gamma\": 0.8},\n",
    "ctr_trainer.fit(train_dataloader, val_dataloader)\n",
    "auc = ctr_trainer.evaluate(ctr_trainer.model, test_dataloader)\n",
    "print(f'test auc: {auc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
